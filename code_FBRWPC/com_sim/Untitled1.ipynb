{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0ef48ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load cs_snf.py\n",
    "\"\"\" Implementation of Similarity Network Fusion (SNF) based similarity combination\n",
    "    [1] Zhang, Yong, Xiaohua Hu, and Xingpeng Jiang. \"Multi-view clustering of microbiome samples by robust similarity network fusion and spectral clustering.\" IEEE/ACM transactions on computational biology and bioinformatics 14.2 (2015): 264-271.\n",
    "    [2] Olayan, Rawan S., Haitham Ashoor, and Vladimir B. Bajic. \"DDR: efficient computational method to predict drug–target interactions using graph mining and machine learning approaches.\" Bioinformatics 34.7 (2018): 1164-1173.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from combine_sims import Combine_Sims_Ave\n",
    "from mv_model_trans.SNF import SNF\n",
    "\n",
    "\n",
    "class Combine_Sims_SNF(Combine_Sims_Ave):\n",
    "    \"\"\" !!! SNF is used for transductive model only\"\"\"\n",
    "    def __init__(self, k=3, num_iters=2, alpha = 1):\n",
    "           super().__init__()\n",
    "           self.k = k #the number of neareast neighbours\n",
    "           self.num_iters = num_iters # the number of iterations\n",
    "           self.alpha = alpha # paramter for normalized function\n",
    "           \n",
    "           self.copyable_attrs=self.copyable_attrs+['k','num_iters','alpha']\n",
    "    #----------------------------------------------------------------------------------------     \n",
    "    \n",
    "    def combine(self, Ss, Y):\n",
    "        if Ss.shape[0] == 1:\n",
    "            S = Ss[0]\n",
    "        else:\n",
    "            S = SNF(Ss, self.k, self.num_iters, self.alpha)\n",
    "        w = np.zeros(Ss.shape[0])\n",
    "        return S, w\n",
    "    #----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0d67e3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "S1 = np.loadtxt(\"./datasets_mv/luo/Dsim/luo_simmat_drugs_ddi.txt\")\n",
    "S2 = np.loadtxt(\"./datasets_mv/luo/Dsim/luo_simmat_drugs_disease.txt\")\n",
    "S3 = np.loadtxt(\"./datasets_mv/luo/Dsim/luo_simmat_drugs_se.txt\")\n",
    "S4 = np.loadtxt(\"./datasets_mv/luo/Dsim/luo_simmat_drugs_tanimoto.txt\")\n",
    "Y_ = np.loadtxt(\"./datasets_mv/luo/luo_admat_dgc.txt\").T\n",
    "similarity_matrices = np.array([S1, S2, S3, S4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b0609ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "com_snf = Combine_Sims_SNF(k=3, num_iters=2, alpha = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c149bea9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "S_com_snf_d,w = com_snf.combine(similarity_matrices, Y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7c7984d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.savetxt('S_com_snf_d.txt', S_com_snf_d, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c7f344",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5c71dbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "S2_p = np.loadtxt(\"./datasets_mv/luo/Tsim/luo_simmat_proteins_disease.txt\")\n",
    "S1_p = np.loadtxt(\"./datasets_mv/luo/Tsim/luo_simmat_proteins_ppi.txt\")\n",
    "S3_p = np.loadtxt(\"./datasets_mv/luo/Tsim/luo_simmat_proteins_sw-n.txt\")\n",
    "\n",
    "Y_1 = np.loadtxt(\"./datasets_mv/luo/luo_admat_dgc.txt\")\n",
    "\n",
    "similarity_matrices_p = np.array([S1_p, S2_p,S3_p])\n",
    "\n",
    "S_com_snf_p,w =com_snf.combine(similarity_matrices_p,Y_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "aba5348c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('S_com_snf_p.txt', S_com_snf_p, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b7f865",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0342417e",
   "metadata": {},
   "source": [
    "# HSIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "11268aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load hsic.py\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize,Bounds\n",
    "from combine_sims import Combine_Sims_Ave\n",
    "\n",
    "\n",
    "class HSIC(Combine_Sims_Ave):\n",
    "    \"\"\" \n",
    "    Impelementation of Hilbert–Schmidt independence criterion-based multiple similarities fusion based on\n",
    "    matlab codes of [1] which is availble at https://figshare.com/s/f664b36119c60e7f6f30\n",
    "    [1] Ding, Yijie, Jijun Tang, and Fei Guo. \"Identification of drug–target interactions via \n",
    "    dual laplacian regularized least squares with multiple kernel fusion.\" Knowledge-Based Systems 204 (2020): 106254.\n",
    "    \"\"\"\n",
    "    def __init__(self, v1=2**-1, v2=2**-4, seed=0):\n",
    "        super().__init__()\n",
    "        self.v1 = v1\n",
    "        self.v2 = v2\n",
    "        self.seed = seed\n",
    "        self.copyable_attrs=self.copyable_attrs+['v1','v2','seed']\n",
    "        \"\"\"\n",
    "        v1 = [2**0, 2**-1, ..., 2**-5]\n",
    "        v2 = [2**0, 2**-1, ..., 2**-5]\n",
    "        \"\"\"\n",
    "    #----------------------------------------------------------------------------------------     \n",
    "        \n",
    "    def _compute_weights(self, Ss, Y):\n",
    "        self._num_sims = Ss.shape[0]\n",
    "        n = Ss.shape[1] # the number of rows in Ss[0]\n",
    "        Ss1 = np.zeros(Ss.shape)\n",
    "        for i in range(self._num_sims):\n",
    "            Ss1[i] = self._process_sim(Ss[i])\n",
    "        S_ideal = Y@Y.T # U in paper\n",
    "        S_ideal = self._normalize_sim(S_ideal)\n",
    "        \n",
    "        H = np.eye(n)-np.ones(Ss1[0].shape, dtype=float)/n\n",
    "        M = np.zeros((self._num_sims,self._num_sims), dtype=float) # the similarity between input similarity matrices\n",
    "        for i in range(self._num_sims):\n",
    "            for j in range(i,self._num_sims):\n",
    "                mm = self._alignment(Ss1[i],Ss1[j])\n",
    "                m1 = self._alignment(Ss1[i],Ss1[i])\n",
    "                m2 = self._alignment(Ss1[j],Ss1[j])\n",
    "                ss = mm/(np.sqrt(m1)*np.sqrt(m2))\n",
    "                M[i,j] = M[j,i] = ss\n",
    "        d1 = np.sum(M, axis=1)\n",
    "        D1 = np.diag(d1)\n",
    "        LapM = D1-M\n",
    "        \n",
    "        a = np.zeros(self._num_sims)\n",
    "        for i in range(self._num_sims):\n",
    "            kk = H@Ss1[i]@H\n",
    "            aa = np.trace(kk.T@S_ideal)\n",
    "            a[i] = (n**-2)*aa # n-1 in matlab code\n",
    "        \n",
    "        prng = np.random.RandomState(self.seed)\n",
    "        w = prng.rand(self._num_sims)\n",
    "        w = w/np.sum(w)\n",
    "        bnds = Bounds(np.zeros(self._num_sims),np.ones(self._num_sims)) # eq.14c\n",
    "        cons = ({'type': 'eq', \"fun\": self._constraint_eq }) # eq.14d\n",
    "        res = minimize(self._f_obj, w, args=(a,LapM), method='SLSQP', bounds=bnds, constraints=cons)\n",
    "        w = res.x\n",
    "        return w\n",
    "    #----------------------------------------------------------------------------------------  \n",
    "    def _process_sim(self, S):\n",
    "        # make similarity matrix symmetric\n",
    "        S1 = (S+S.T)/2 \n",
    "        # make similarity matrix PSD\n",
    "        eig_values = np.linalg.eigvals(S)\n",
    "        eig_values = np.real_if_close(eig_values) # keep the real part of eig_values\n",
    "        ev_min = np.min(eig_values)\n",
    "        e = max(0.0, -1.0*ev_min+1e-4)\n",
    "        e1 = e.real\n",
    "        S1 = S1 + e1*np.eye(S.shape[0])\n",
    "        \n",
    "        S1 = self._normalize_sim(S1)\n",
    "        return S1\n",
    "    \n",
    "    def _normalize_sim(self, S):\n",
    "        min_nz = np.min(S[np.nonzero(S)]) # the mininal none zero value\n",
    "        S[S==0] = min_nz\n",
    "        D = np.diag(S)\n",
    "        D = np.sqrt(D)\n",
    "        S1 = S/(np.outer(D,D)) \n",
    "        return S1\n",
    "    #----------------------------------------------------------------------------------------  \n",
    "    \n",
    "    def _alignment(self, S1, S2):\n",
    "        # same with np.trace(S1.T@S2)\n",
    "        A = S1*S2\n",
    "        a = A.sum()\n",
    "        return a     \n",
    "    #----------------------------------------------------------------------------------------  \n",
    "    \n",
    "    def _f_obj(self, w, a, LapM):\n",
    "        #  eq.14a.\n",
    "        J = -1*w@a + self.v1*w.T@LapM@w +self.v2*np.linalg.norm(w,2)**2 # last term is equalient to w@w\n",
    "        return J\n",
    "    #----------------------------------------------------------------------------------------\n",
    "    \n",
    "    def _constraint_eq(self, w):\n",
    "        \"\"\"\n",
    "        return value must come back as 0 to be accepted \n",
    "        if return value is anything other than 0 it's rejectedas not a valid answer.\n",
    "        \"\"\"\n",
    "        s = np.sum(w)-1\n",
    "        return s\n",
    "    #----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ef0ede8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "com_hsic = HSIC(v1=2**-1, v2=2**-4, seed=0)\n",
    "\n",
    "S1 = np.loadtxt(\"./datasets_mv/luo/Dsim/luo_simmat_drugs_ddi.txt\")\n",
    "S2 = np.loadtxt(\"./datasets_mv/luo/Dsim/luo_simmat_drugs_disease.txt\")\n",
    "S3 = np.loadtxt(\"./datasets_mv/luo/Dsim/luo_simmat_drugs_se.txt\")\n",
    "S4 = np.loadtxt(\"./datasets_mv/luo/Dsim/luo_simmat_drugs_tanimoto.txt\")\n",
    "Y_ = np.loadtxt(\"./datasets_mv/luo/luo_admat_dgc.txt\").T\n",
    "similarity_matrices = np.array([S1, S2, S3, S4])\n",
    "\n",
    "com_hsic._compute_weights(similarity_matrices,Y_)\n",
    "\n",
    "\n",
    "S_com_hsic_d = 0.2501135*S1+0.24991581*S2+0.24992715*S3+0.25004353*S4\n",
    "np.savetxt('S_com_hsic_d.txt', S_com_hsic_d, delimiter='\\t')\n",
    "com_hsic._process_sim(S_com_hsic)\n",
    "\n",
    "com_hsic._normalize_sim(S_com_hsic)\n",
    "\n",
    "com_hsic._constraint_eq(com_hsic._compute_weights(similarity_matrices,Y_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ae90b4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "S2_p = np.loadtxt(\"./datasets_mv/luo/Tsim/luo_simmat_proteins_disease.txt\")\n",
    "S1_p = np.loadtxt(\"./datasets_mv/luo/Tsim/luo_simmat_proteins_ppi.txt\")\n",
    "S3_p = np.loadtxt(\"./datasets_mv/luo/Tsim/luo_simmat_proteins_sw-n.txt\")\n",
    "\n",
    "Y_1 = np.loadtxt(\"./datasets_mv/luo/luo_admat_dgc.txt\")\n",
    "\n",
    "similarity_matrices_p = np.array([S1_p, S2_p,S3_p])\n",
    "\n",
    "com_hsic._compute_weights(similarity_matrices_p,Y_1)\n",
    "\n",
    "S_com_hsic_p = 0.33333129*S2_p+0.33335728*S1_p+0.33331143*S3_p\n",
    "np.savetxt('S_com_hsic_p.txt', S_com_hsic_p, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3242a78a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3839792d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf160559",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a17be810",
   "metadata": {},
   "source": [
    "# LIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b5cd2086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load combine_sims.py\n",
    "import numpy as np\n",
    "from base.csbase import Combine_Sims_Base\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# class Combine_Sims_base:\n",
    "#     def __init__(self):\n",
    "#         pass\n",
    "# #---------------------------------------------------------------------------------------- \n",
    "        \n",
    "#     def combine(self, Ss, Y=None):\n",
    "#         self._num_sims = Ss.shape[0]\n",
    "#         w = self._compute_weights()\n",
    "#         S = np.average(Ss,axis=0,weights=w) # aggragate weights based on w\n",
    "#         return S, w\n",
    "    \n",
    "#     def _compute_weights(self):\n",
    "#         w = np.full(self._num_sims,1.0/self._num_sims, dtype=float) # the sum of weights could not be one.\n",
    "#         return w\n",
    "\n",
    "class Combine_Sims_Ave(Combine_Sims_Base):\n",
    "    \"\"\" \n",
    "    using equal wieghts and the weight of each similairty is 1/num_sims\n",
    "    A multiple kernel learning algorithm for drug-target interaction prediction  BMC Bioinf 16\n",
    "    It aslo the base class of all Combine_Sims classes\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.copyable_attrs  = []\n",
    "#---------------------------------------------------------------------------------------- \n",
    "        \n",
    "    def combine(self, Ss, Y):\n",
    "        self._num_sims = Ss.shape[0]\n",
    "        w = self._compute_weights(Ss, Y)\n",
    "        S = np.average(Ss,axis=0,weights=w) # aggragate similarity based on w\n",
    "        return S, w\n",
    "#---------------------------------------------------------------------------------------- \n",
    "        \n",
    "    def _compute_weights(self, Ss, Y): # Ss and Y are not used in this function\n",
    "        w = np.full(self._num_sims,1.0/self._num_sims, dtype=float) # the sum of weights could not be one.\n",
    "        return w\n",
    "#----------------------------------------------------------------------------------------   \n",
    "#----------------------------------------------------------------------------------------          \n",
    "\n",
    "\n",
    "class Combine_Sims_KA(Combine_Sims_Ave):\n",
    "    \"\"\" \n",
    "    kernel alignment (KA) heuristics weighting method. \n",
    "    The weights of a similalrity is propotional to its aligment to the idal kernel\n",
    "    A multiple kernel learning algorithm for drug-target interaction prediction  BMC Bioinf 16\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "#---------------------------------------------------------------------------------------- \n",
    "        \n",
    "    def _compute_weights(self, Ss, Y):\n",
    "        n = Ss.shape[1] # the number of rows in Ss[0]\n",
    "        w = np.zeros(self._num_sims, dtype=float) \n",
    "        S_ideal = Y@Y.T\n",
    "        for i in range(self._num_sims):\n",
    "            w[i] = self._alignment(Ss[i],S_ideal)/(n*np.sqrt(self._alignment(Ss[i],Ss[i])))\n",
    "        w = w/np.sum(w)\n",
    "        return w\n",
    "#----------------------------------------------------------------------------------------     \n",
    "        \n",
    "    def _alignment(self, S1, S2):\n",
    "        A = S1*S2\n",
    "        a = A.sum()\n",
    "        return a     \n",
    "#----------------------------------------------------------------------------------------   \n",
    "#----------------------------------------------------------------------------------------   \n",
    "\n",
    "class Combine_Sims_KA2(Combine_Sims_KA):\n",
    "    \"\"\" \n",
    "    Difference with Combine_Sims_KA: S only contain k largest values in each row excluding the diagonal elements \n",
    "    \"\"\"\n",
    "    def __init__(self, k=5):\n",
    "        super().__init__()\n",
    "        self.k = k\n",
    "        self.copyable_attrs.append('k')\n",
    "#---------------------------------------------------------------------------------------- \n",
    "        \n",
    "    def _compute_weights(self, Ss, Y):\n",
    "        n = Ss.shape[1] # the number of rows in Ss[0]\n",
    "        w = np.zeros(self._num_sims, dtype=float) \n",
    "        neigh = NearestNeighbors(n_neighbors=self.k, metric='precomputed')\n",
    "        neigh.fit(np.zeros(Ss[0].shape))\n",
    "                \n",
    "        S_ideal = Y@Y.T\n",
    "          \n",
    "        for i in range(self._num_sims):\n",
    "            S = Ss[i] - np.diag(np.diag(Ss[i])) # set diagnol elements to zeros\n",
    "            knn = neigh.kneighbors(1-S, return_distance=False)\n",
    "            S1 = np.zeros(S.shape)\n",
    "            for j in range(S.shape[0]):\n",
    "                jj = knn[j]\n",
    "                S1[j, jj] = S[j, jj]\n",
    "            w[i] = self._alignment(S1,S_ideal)/(n*np.sqrt(self._alignment(S1,S1)))\n",
    "        w = w/np.sum(w)\n",
    "        return w\n",
    "#----------------------------------------------------------------------------------------     \n",
    "        \n",
    "    def _alignment(self, S1, S2):\n",
    "        A = S1*S2\n",
    "        a = A.sum()\n",
    "        return a     \n",
    "#----------------------------------------------------------------------------------------   \n",
    "#----------------------------------------------------------------------------------------   \n",
    "    \n",
    "        \n",
    "class Combine_Sims_Limb(Combine_Sims_Ave):\n",
    "    \"\"\" \n",
    "    the weight is proprotional to the 1-local imbalance\n",
    "    \"\"\"\n",
    "    def __init__(self, k=5):\n",
    "        super().__init__()\n",
    "        self.k = k\n",
    "        self.copyable_attrs.append('k')    \n",
    "#---------------------------------------------------------------------------------------- \n",
    "        \n",
    "    def _compute_weights(self, Ss, Y):\n",
    "        self._num_sims = Ss.shape[0]\n",
    "        w = np.zeros(self._num_sims, dtype=float) \n",
    "        for i in range(self._num_sims):\n",
    "            S1 = Ss[i] - np.diag(np.diag(Ss[i])) # set diagnol elements to zeros\n",
    "            w[i] = 1-self._cal_limb(S1, Y, self.k)\n",
    "        w = w/np.sum(w)\n",
    "        return w \n",
    "#----------------------------------------------------------------------------------------         \n",
    "        \n",
    "    def _cal_limb(self, S, Y, k):\n",
    "        \"\"\" S is similarity matrix whose dignoal elememets are zeros\"\"\"\n",
    "        \n",
    "        neigh = NearestNeighbors(n_neighbors=k, metric='precomputed')\n",
    "        neigh.fit(np.zeros(S.shape))\n",
    "        knns = neigh.kneighbors(1 - S, return_distance=False)\n",
    "        \n",
    "        C = np.zeros(Y.shape, dtype=float)\n",
    "        for i in range(Y.shape[0]):\n",
    "            ii = knns[i]\n",
    "            for j in range(Y.shape[1]):\n",
    "                if Y[i,j] == 1: # only consider \"1\" \n",
    "                    C[i,j] = k-np.sum(Y[ii,j])\n",
    "        C = C/k\n",
    "        milb = np.sum(C)/np.sum(Y)\n",
    "        \n",
    "        return milb\n",
    "#----------------------------------------------------------------------------------------   \n",
    "#----------------------------------------------------------------------------------------\n",
    "        \n",
    "    \n",
    "class Combine_Sims_Limb2(Combine_Sims_Limb):\n",
    "    \"\"\" \n",
    "    Difference with Combine_Sims_Limb\n",
    "    the weight is proprotional to the 1/(local imbalance)\n",
    "    \"\"\"\n",
    "    def __init__(self, k=5):\n",
    "        super().__init__(k)\n",
    "#---------------------------------------------------------------------------------------- \n",
    "        \n",
    "    def _compute_weights(self, Ss, Y):\n",
    "        w = np.zeros(self._num_sims, dtype=float) \n",
    "        for i in range(self._num_sims):\n",
    "            S1 = Ss[i] - np.diag(np.diag(Ss[i])) # set diagnol elements to zeros\n",
    "            w[i] = 1/self._cal_limb(S1, Y, self.k)\n",
    "        w = w/np.sum(w)\n",
    "        return w \n",
    "#----------------------------------------------------------------------------------------   \n",
    "#----------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "class Combine_Sims_Limb3(Combine_Sims_Limb):\n",
    "    \"\"\" \n",
    "    Difference with Combine_Sims_Limb\n",
    "    considering the influence of similarities\n",
    "    !!!!!! This is one finally used in the paper\n",
    "    \"\"\"\n",
    "    def __init__(self, k=5):\n",
    "        super().__init__(k)\n",
    "        \n",
    "#---------------------------------------------------------------------------------------- \n",
    "    \n",
    "    def _cal_limb(self, S, Y, k):\n",
    "        \"\"\" S is similarity matrix whose dignoal elememets are zeros\"\"\"\n",
    "        \n",
    "        neigh = NearestNeighbors(n_neighbors=k, metric='precomputed')\n",
    "        neigh.fit(np.zeros(S.shape))\n",
    "        idx = np.where(S>1.0)\n",
    "        knns = neigh.kneighbors(1 - S, return_distance=False)\n",
    "                \n",
    "        C = np.zeros(Y.shape, dtype=float)\n",
    "        for i in range(Y.shape[0]):\n",
    "            ii = knns[i]\n",
    "            s = S[i,ii]\n",
    "            z = np.sum(s)\n",
    "            if z == 0:\n",
    "                z=1\n",
    "            C[i] = 1-s@Y[ii,:]/z\n",
    "        C *= Y\n",
    "        milb = np.sum(C)/np.sum(Y)\n",
    "        \n",
    "        return milb\n",
    "#---------------------------------------------------------------------------------------- \n",
    "#----------------------------------------------------------------------------------------\n",
    "        \n",
    "\n",
    "class Combine_Sims_LowestLimb(Combine_Sims_Limb):\n",
    "    \"\"\" \n",
    "    choose the one similarity with lowerest local imbalance\n",
    "    \"\"\"\n",
    "    def __init__(self, k=5):\n",
    "        super().__init__(k)\n",
    "#---------------------------------------------------------------------------------------- \n",
    "    \n",
    "    def _compute_weights(self, Ss, Y):\n",
    "        w = np.zeros(self._num_sims, dtype=float) \n",
    "        limb_ds = np.ones(self._num_sims, dtype=float)\n",
    "        for i in range(self._num_sims):\n",
    "            S1 = Ss[i] - np.diag(np.diag(Ss[i])) # set diagnol elements to zeros\n",
    "            limb_ds[i] = self._cal_limb(S1, Y, self.k)\n",
    "        min_idx = np.argmin(limb_ds)\n",
    "        w[min_idx] = 1.0\n",
    "        # w = w/np.sum(w)\n",
    "        return w \n",
    "#----------------------------------------------------------------------------------------\n",
    "\n",
    "class Combine_Sims_Single(Combine_Sims_Ave):\n",
    "    \"\"\" \n",
    "    Only use one single similarity\n",
    "    \"\"\"\n",
    "    def __init__(self, ind=0):\n",
    "        super().__init__()\n",
    "        self.ind = ind # ind similarity is used, others' weights are set as 0\n",
    "        self.copyable_attrs.append('ind')\n",
    "#---------------------------------------------------------------------------------------- \n",
    "    \n",
    "    def _compute_weights(self, Ss, Y):\n",
    "        w = np.zeros(self._num_sims, dtype=float) \n",
    "        w[self.ind] = 1.0\n",
    "        return w \n",
    "#----------------------------------------------------------------------------------------\n",
    "\n",
    "#----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "96b078d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "com_lic = Combine_Sims_Limb3(k=5)\n",
    "S1 = np.loadtxt(\"./datasets_mv/luo/Dsim/luo_simmat_drugs_ddi.txt\")\n",
    "S2 = np.loadtxt(\"./datasets_mv/luo/Dsim/luo_simmat_drugs_disease.txt\")\n",
    "S3 = np.loadtxt(\"./datasets_mv/luo/Dsim/luo_simmat_drugs_se.txt\")\n",
    "S4 = np.loadtxt(\"./datasets_mv/luo/Dsim/luo_simmat_drugs_tanimoto.txt\")\n",
    "Y_ = np.loadtxt(\"./datasets_mv/luo/luo_admat_dgc.txt\").T\n",
    "similarity_matrices = np.array([S1, S2, S3, S4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a1e36df1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.2752286 , 0.16697413, 0.24368714, 0.31411014])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "com_lic._compute_weights(similarity_matrices,Y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f49dd320",
   "metadata": {},
   "outputs": [],
   "source": [
    "S_com_lic_d = 0.2752286*S1+0.16697413*S2+0.24368714*S3+0.31411014*S4\n",
    "\n",
    "\n",
    "np.savetxt('S_com_lic_d.txt', S_com_lic_d, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6cda2ec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.19642949, 0.19249905, 0.61107146])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S2_p = np.loadtxt(\"./datasets_mv/luo/Tsim/luo_simmat_proteins_disease.txt\")\n",
    "S1_p = np.loadtxt(\"./datasets_mv/luo/Tsim/luo_simmat_proteins_ppi.txt\")\n",
    "S3_p = np.loadtxt(\"./datasets_mv/luo/Tsim/luo_simmat_proteins_sw-n.txt\")\n",
    "\n",
    "Y_1 = np.loadtxt(\"./datasets_mv/luo/luo_admat_dgc.txt\")\n",
    "\n",
    "similarity_matrices_p = np.array([S1_p, S2_p,S3_p])\n",
    "\n",
    "com_lic._compute_weights(similarity_matrices_p,Y_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b5360e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "S_com_lic_p = 0.19642949*S2_p+0.19249905*S1_p+0.61107146*S3_p\n",
    "\n",
    "\n",
    "np.savetxt('S_com_lic_p.txt', S_com_lic_p, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b156c6f",
   "metadata": {},
   "source": [
    "# KA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4a045efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load combine_sims.py\n",
    "import numpy as np\n",
    "from base.csbase import Combine_Sims_Base\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# class Combine_Sims_base:\n",
    "#     def __init__(self):\n",
    "#         pass\n",
    "# #---------------------------------------------------------------------------------------- \n",
    "        \n",
    "#     def combine(self, Ss, Y=None):\n",
    "#         self._num_sims = Ss.shape[0]\n",
    "#         w = self._compute_weights()\n",
    "#         S = np.average(Ss,axis=0,weights=w) # aggragate weights based on w\n",
    "#         return S, w\n",
    "    \n",
    "#     def _compute_weights(self):\n",
    "#         w = np.full(self._num_sims,1.0/self._num_sims, dtype=float) # the sum of weights could not be one.\n",
    "#         return w\n",
    "\n",
    "class Combine_Sims_Ave(Combine_Sims_Base):\n",
    "    \"\"\" \n",
    "    using equal wieghts and the weight of each similairty is 1/num_sims\n",
    "    A multiple kernel learning algorithm for drug-target interaction prediction  BMC Bioinf 16\n",
    "    It aslo the base class of all Combine_Sims classes\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.copyable_attrs  = []\n",
    "#---------------------------------------------------------------------------------------- \n",
    "        \n",
    "    def combine(self, Ss, Y):\n",
    "        self._num_sims = Ss.shape[0]\n",
    "        w = self._compute_weights(Ss, Y)\n",
    "        S = np.average(Ss,axis=0,weights=w) # aggragate similarity based on w\n",
    "        return S, w\n",
    "#---------------------------------------------------------------------------------------- \n",
    "        \n",
    "    def _compute_weights(self, Ss, Y): # Ss and Y are not used in this function\n",
    "        w = np.full(self._num_sims,1.0/self._num_sims, dtype=float) # the sum of weights could not be one.\n",
    "        return w\n",
    "#----------------------------------------------------------------------------------------   \n",
    "#----------------------------------------------------------------------------------------          \n",
    "\n",
    "\n",
    "class Combine_Sims_KA(Combine_Sims_Ave):\n",
    "    \"\"\" \n",
    "    kernel alignment (KA) heuristics weighting method. \n",
    "    The weights of a similalrity is propotional to its aligment to the idal kernel\n",
    "    A multiple kernel learning algorithm for drug-target interaction prediction  BMC Bioinf 16\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "#---------------------------------------------------------------------------------------- \n",
    "        \n",
    "    def _compute_weights(self, Ss, Y):\n",
    "        self._num_sims = Ss.shape[0]\n",
    "        n = Ss.shape[1] # the number of rows in Ss[0]\n",
    "        w = np.zeros(self._num_sims, dtype=float) \n",
    "        S_ideal = Y@Y.T\n",
    "        for i in range(self._num_sims):\n",
    "            w[i] = self._alignment(Ss[i],S_ideal)/(n*np.sqrt(self._alignment(Ss[i],Ss[i])))\n",
    "        w = w/np.sum(w)\n",
    "        return w\n",
    "#----------------------------------------------------------------------------------------     \n",
    "        \n",
    "    def _alignment(self, S1, S2):\n",
    "        A = S1*S2\n",
    "        a = A.sum()\n",
    "        return a     \n",
    "#----------------------------------------------------------------------------------------   \n",
    "#----------------------------------------------------------------------------------------   \n",
    "\n",
    "class Combine_Sims_KA2(Combine_Sims_KA):\n",
    "    \"\"\" \n",
    "    Difference with Combine_Sims_KA: S only contain k largest values in each row excluding the diagonal elements \n",
    "    \"\"\"\n",
    "    def __init__(self, k=5):\n",
    "        super().__init__()\n",
    "        self.k = k\n",
    "        self.copyable_attrs.append('k')\n",
    "#---------------------------------------------------------------------------------------- \n",
    "        \n",
    "    def _compute_weights(self, Ss, Y):\n",
    "        n = Ss.shape[1] # the number of rows in Ss[0]\n",
    "        w = np.zeros(self._num_sims, dtype=float) \n",
    "        neigh = NearestNeighbors(n_neighbors=self.k, metric='precomputed')\n",
    "        neigh.fit(np.zeros(Ss[0].shape))\n",
    "                \n",
    "        S_ideal = Y@Y.T\n",
    "          \n",
    "        for i in range(self._num_sims):\n",
    "            S = Ss[i] - np.diag(np.diag(Ss[i])) # set diagnol elements to zeros\n",
    "            knn = neigh.kneighbors(1-S, return_distance=False)\n",
    "            S1 = np.zeros(S.shape)\n",
    "            for j in range(S.shape[0]):\n",
    "                jj = knn[j]\n",
    "                S1[j, jj] = S[j, jj]\n",
    "            w[i] = self._alignment(S1,S_ideal)/(n*np.sqrt(self._alignment(S1,S1)))\n",
    "        w = w/np.sum(w)\n",
    "        return w\n",
    "#----------------------------------------------------------------------------------------     \n",
    "        \n",
    "    def _alignment(self, S1, S2):\n",
    "        A = S1*S2\n",
    "        a = A.sum()\n",
    "        return a     \n",
    "#----------------------------------------------------------------------------------------   \n",
    "#----------------------------------------------------------------------------------------   \n",
    "    \n",
    "        \n",
    "class Combine_Sims_Limb(Combine_Sims_Ave):\n",
    "    \"\"\" \n",
    "    the weight is proprotional to the 1-local imbalance\n",
    "    \"\"\"\n",
    "    def __init__(self, k=5):\n",
    "        super().__init__()\n",
    "        self.k = k\n",
    "        self.copyable_attrs.append('k')    \n",
    "#---------------------------------------------------------------------------------------- \n",
    "        \n",
    "    def _compute_weights(self, Ss, Y):\n",
    "        self._num_sims = Ss.shape[0]\n",
    "        w = np.zeros(self._num_sims, dtype=float) \n",
    "        for i in range(self._num_sims):\n",
    "            S1 = Ss[i] - np.diag(np.diag(Ss[i])) # set diagnol elements to zeros\n",
    "            w[i] = 1-self._cal_limb(S1, Y, self.k)\n",
    "        w = w/np.sum(w)\n",
    "        return w \n",
    "#----------------------------------------------------------------------------------------         \n",
    "        \n",
    "    def _cal_limb(self, S, Y, k):\n",
    "        \"\"\" S is similarity matrix whose dignoal elememets are zeros\"\"\"\n",
    "        \n",
    "        neigh = NearestNeighbors(n_neighbors=k, metric='precomputed')\n",
    "        neigh.fit(np.zeros(S.shape))\n",
    "        knns = neigh.kneighbors(1 - S, return_distance=False)\n",
    "        \n",
    "        C = np.zeros(Y.shape, dtype=float)\n",
    "        for i in range(Y.shape[0]):\n",
    "            ii = knns[i]\n",
    "            for j in range(Y.shape[1]):\n",
    "                if Y[i,j] == 1: # only consider \"1\" \n",
    "                    C[i,j] = k-np.sum(Y[ii,j])\n",
    "        C = C/k\n",
    "        milb = np.sum(C)/np.sum(Y)\n",
    "        \n",
    "        return milb\n",
    "#----------------------------------------------------------------------------------------   \n",
    "#----------------------------------------------------------------------------------------\n",
    "        \n",
    "    \n",
    "class Combine_Sims_Limb2(Combine_Sims_Limb):\n",
    "    \"\"\" \n",
    "    Difference with Combine_Sims_Limb\n",
    "    the weight is proprotional to the 1/(local imbalance)\n",
    "    \"\"\"\n",
    "    def __init__(self, k=5):\n",
    "        super().__init__(k)\n",
    "#---------------------------------------------------------------------------------------- \n",
    "        \n",
    "    def _compute_weights(self, Ss, Y):\n",
    "        w = np.zeros(self._num_sims, dtype=float) \n",
    "        for i in range(self._num_sims):\n",
    "            S1 = Ss[i] - np.diag(np.diag(Ss[i])) # set diagnol elements to zeros\n",
    "            w[i] = 1/self._cal_limb(S1, Y, self.k)\n",
    "        w = w/np.sum(w)\n",
    "        return w \n",
    "#----------------------------------------------------------------------------------------   \n",
    "#----------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "class Combine_Sims_Limb3(Combine_Sims_Limb):\n",
    "    \"\"\" \n",
    "    Difference with Combine_Sims_Limb\n",
    "    considering the influence of similarities\n",
    "    !!!!!! This is one finally used in the paper\n",
    "    \"\"\"\n",
    "    def __init__(self, k=5):\n",
    "        super().__init__(k)\n",
    "        \n",
    "#---------------------------------------------------------------------------------------- \n",
    "    \n",
    "    def _cal_limb(self, S, Y, k):\n",
    "        \"\"\" S is similarity matrix whose dignoal elememets are zeros\"\"\"\n",
    "        \n",
    "        neigh = NearestNeighbors(n_neighbors=k, metric='precomputed')\n",
    "        neigh.fit(np.zeros(S.shape))\n",
    "        idx = np.where(S>1.0)\n",
    "        knns = neigh.kneighbors(1 - S, return_distance=False)\n",
    "                \n",
    "        C = np.zeros(Y.shape, dtype=float)\n",
    "        for i in range(Y.shape[0]):\n",
    "            ii = knns[i]\n",
    "            s = S[i,ii]\n",
    "            z = np.sum(s)\n",
    "            if z == 0:\n",
    "                z=1\n",
    "            C[i] = 1-s@Y[ii,:]/z\n",
    "        C *= Y\n",
    "        milb = np.sum(C)/np.sum(Y)\n",
    "        \n",
    "        return milb\n",
    "#---------------------------------------------------------------------------------------- \n",
    "#----------------------------------------------------------------------------------------\n",
    "        \n",
    "\n",
    "class Combine_Sims_LowestLimb(Combine_Sims_Limb):\n",
    "    \"\"\" \n",
    "    choose the one similarity with lowerest local imbalance\n",
    "    \"\"\"\n",
    "    def __init__(self, k=5):\n",
    "        super().__init__(k)\n",
    "#---------------------------------------------------------------------------------------- \n",
    "    \n",
    "    def _compute_weights(self, Ss, Y):\n",
    "        w = np.zeros(self._num_sims, dtype=float) \n",
    "        limb_ds = np.ones(self._num_sims, dtype=float)\n",
    "        for i in range(self._num_sims):\n",
    "            S1 = Ss[i] - np.diag(np.diag(Ss[i])) # set diagnol elements to zeros\n",
    "            limb_ds[i] = self._cal_limb(S1, Y, self.k)\n",
    "        min_idx = np.argmin(limb_ds)\n",
    "        w[min_idx] = 1.0\n",
    "        # w = w/np.sum(w)\n",
    "        return w \n",
    "#----------------------------------------------------------------------------------------\n",
    "\n",
    "class Combine_Sims_Single(Combine_Sims_Ave):\n",
    "    \"\"\" \n",
    "    Only use one single similarity\n",
    "    \"\"\"\n",
    "    def __init__(self, ind=0):\n",
    "        super().__init__()\n",
    "        self.ind = ind # ind similarity is used, others' weights are set as 0\n",
    "        self.copyable_attrs.append('ind')\n",
    "#---------------------------------------------------------------------------------------- \n",
    "    \n",
    "    def _compute_weights(self, Ss, Y):\n",
    "        w = np.zeros(self._num_sims, dtype=float) \n",
    "        w[self.ind] = 1.0\n",
    "        return w \n",
    "#----------------------------------------------------------------------------------------\n",
    "\n",
    "#----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1ffff268",
   "metadata": {},
   "outputs": [],
   "source": [
    "com_KA = Combine_Sims_KA()\n",
    "S1 = np.loadtxt(\"./datasets_mv/luo/Dsim/luo_simmat_drugs_ddi.txt\")\n",
    "S2 = np.loadtxt(\"./datasets_mv/luo/Dsim/luo_simmat_drugs_disease.txt\")\n",
    "S3 = np.loadtxt(\"./datasets_mv/luo/Dsim/luo_simmat_drugs_se.txt\")\n",
    "S4 = np.loadtxt(\"./datasets_mv/luo/Dsim/luo_simmat_drugs_tanimoto.txt\")\n",
    "Y_ = np.loadtxt(\"./datasets_mv/luo/luo_admat_dgc.txt\").T\n",
    "similarity_matrices = np.array([S1, S2, S3, S4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "54a6c285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.35971064, 0.18774675, 0.22692613, 0.22561648])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "com_KA._compute_weights(similarity_matrices,Y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "52517857",
   "metadata": {},
   "outputs": [],
   "source": [
    "S_com_KA_d = 0.35971064*S1+0.18774675*S2+0.22692613*S3+0.22561648*S4\n",
    "\n",
    "\n",
    "np.savetxt('S_com_KA_d.txt', S_com_KA_d, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8de67ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "S2_p = np.loadtxt(\"./datasets_mv/luo/Tsim/luo_simmat_proteins_disease.txt\")\n",
    "S1_p = np.loadtxt(\"./datasets_mv/luo/Tsim/luo_simmat_proteins_ppi.txt\")\n",
    "S3_p = np.loadtxt(\"./datasets_mv/luo/Tsim/luo_simmat_proteins_sw-n.txt\")\n",
    "\n",
    "Y_1 = np.loadtxt(\"./datasets_mv/luo/luo_admat_dgc.txt\")\n",
    "\n",
    "similarity_matrices_p = np.array([S1_p, S2_p,S3_p])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1fff3f1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.67541219, 0.1758271 , 0.14876071])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "com_KA._compute_weights(similarity_matrices_p,Y_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "54502e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "S_com_KA_p = 0.67541219*S2_p+0.1758271 *S1_p+0.14876071*S3_p\n",
    "\n",
    "\n",
    "np.savetxt('S_com_KA_p.txt', S_com_KA_p, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9affb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
